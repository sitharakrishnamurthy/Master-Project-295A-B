"# MediaPipe graph that performs hair segmentation with TensorFlow Lite on CPU.\n# Used in the example in\n# mediapipie/examples/android/src/java/com/mediapipe/apps/hairsegmentationgpu,\n# and then ported over for CPU ML and web-side usage.\n\n# Images on GPU coming into and out of the graph.\ninput_stream: \"input_frames_gpu\"\noutput_stream: \"output_frames_gpu\"\n\nmax_queue_size: 100\n\nexecutor: {\n  name: \"\"\n  type: \"ApplicationThreadExecutor\"\n}\n\n# For selfie-mode testing, we flip horizontally here.\nnode: {\n  calculator: \"ImageTransformationCalculator\"\n  input_stream: \"IMAGE_GPU:input_frames_gpu\"\n  output_stream: \"IMAGE_GPU:flipped_input_video\"\n  node_options: {\n    [type.googleapis.com/mediapipe.ImageTransformationCalculatorOptions]: {\n      flip_horizontally: true\n    }\n  }\n}\n\n# Transforms the input image on GPU to a 512x512 image. To scale the image, by\n# default it uses the STRETCH scale mode that maps the entire input image to the\n# entire transformed image. As a result, image aspect ratio may be changed and\n# objects in the image may be deformed (stretched or squeezed), but the hair\n# segmentation model used in this graph is agnostic to that deformation.\nnode: {\n  calculator: \"ImageTransformationCalculator\"\n  input_stream: \"IMAGE_GPU:flipped_input_video\"\n  output_stream: \"IMAGE_GPU:transformed_input_video\"\n  node_options: {\n    [type.googleapis.com/mediapipe.ImageTransformationCalculatorOptions]: {\n      output_width: 512\n      output_height: 512\n    }\n  }\n}\n\n# Caches a mask fed back from the previous round of hair segmentation, and upon\n# the arrival of the next input image sends out the cached mask with the\n# timestamp replaced by that of the input image, essentially generating a packet\n# that carries the previous mask. Note that upon the arrival of the very first\n# input image, an empty packet is sent out to jump start the feedback loop.\nnode {\n  calculator: \"PreviousLoopbackCalculator\"\n  input_stream: \"MAIN:flipped_input_video\"\n  input_stream: \"LOOP:hair_mask\"\n  input_stream_info: {\n    tag_index: \"LOOP\"\n    back_edge: true\n  }\n  output_stream: \"PREV_LOOP:previous_hair_mask\"\n}\n\nnode {\n  calculator: \"ImageFrameToGpuBufferCalculator\"\n  input_stream: \"previous_hair_mask\"\n  output_stream: \"previous_hair_mask_gpu\"\n}\n\n# Embeds the hair mask generated from the previous round of hair segmentation\n# as the alpha channel of the current input image.\nnode {\n  calculator: \"SetAlphaCalculator\"\n  input_stream: \"IMAGE_GPU:transformed_input_video\"\n  input_stream: \"ALPHA_GPU:previous_hair_mask_gpu\"\n  output_stream: \"IMAGE_GPU:mask_embedded_input_video\"\n}\n\nnode {\n  calculator: \"GpuBufferToImageFrameCalculator\"\n  input_stream: \"mask_embedded_input_video\"\n  output_stream: \"mask_embedded_input_video_cpu\"\n}\n\n# Converts the transformed input image on CPU into an image tensor.\n# The zero_center option is set to false to normalize the\n# pixel values to [0.f, 1.f] as opposed to [-1.f, 1.f]. With the\n# max_num_channels option set to 4, all 4 RGBA channels are contained in the\n# image tensor.\nnode {\n  calculator: \"TfLiteConverterCalculator\"\n  input_stream: \"IMAGE:mask_embedded_input_video_cpu\"\n  output_stream: \"TENSORS:image_tensor\"\n  node_options: {\n    [type.googleapis.com/mediapipe.TfLiteConverterCalculatorOptions]: {\n      zero_center: false\n      max_num_channels: 4\n    }\n  }\n}\n\n# Generates a single side packet containing a TensorFlow Lite op resolver that\n# supports custom ops needed by the model used in this graph.\nnode {\n  calculator: \"TfLiteCustomOpResolverCalculator\"\n  output_side_packet: \"op_resolver\"\n  node_options: {\n    [type.googleapis.com/mediapipe.TfLiteCustomOpResolverCalculatorOptions]: {\n      # Note: \"use_gpu: true\" removed from here.\n    }\n  }\n}\n\n# Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a\n# tensor representing the hair segmentation, which has the same width and height\n# as the input image tensor.\nnode {\n  calculator: \"TfLiteInferenceCalculator\"\n  input_stream: \"TENSORS:image_tensor\"\n  output_stream: \"TENSORS:segmentation_tensor\"\n  input_side_packet: \"CUSTOM_OP_RESOLVER:op_resolver\"\n  node_options: {\n    [type.googleapis.com/mediapipe.TfLiteInferenceCalculatorOptions]: {\n      model_path: \"hair_segmentation.tflite\"\n      # Note: \"use_gpu: true\" removed from here.\n    }\n  }\n}\n\n# Decodes the segmentation tensor generated by the TensorFlow Lite model into a\n# mask of values in [0.f, 1.f], stored in the R channel of a CPU buffer. It also\n# takes the mask generated previously as another input to improve the temporal\n# consistency.\nnode {\n  calculator: \"TfLiteTensorsToSegmentationCalculator\"\n  input_stream: \"TENSORS:segmentation_tensor\"\n  input_stream: \"PREV_MASK:previous_hair_mask\"\n  output_stream: \"MASK:hair_mask\"\n  node_options: {\n    [type.googleapis.com/mediapipe.TfLiteTensorsToSegmentationCalculatorOptions]: {\n      tensor_width: 512\n      tensor_height: 512\n      tensor_channels: 2\n      combine_with_previous_ratio: 0.9\n      output_layer_index: 1\n    }\n  }\n}\n\nnode {\n  calculator: \"ImageFrameToGpuBufferCalculator\"\n  input_stream: \"hair_mask\"\n  output_stream: \"hair_mask_gpu\"\n}\n\n# Colors the hair segmentation with the color specified in the option.\nnode {\n  calculator: \"RecolorCalculator\"\n  input_stream: \"IMAGE_GPU:flipped_input_video\"\n  input_stream: \"MASK_GPU:hair_mask_gpu\"\n  output_stream: \"IMAGE_GPU:output_frames_gpu\"\n  node_options: {\n    [type.googleapis.com/mediapipe.RecolorCalculatorOptions]: {\n      color { r: 0 g: 0 b: 255 }\n      mask_channel: RED\n    }\n  }\n}\n"